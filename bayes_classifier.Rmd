---
editor_options:
  markdown:
    wrap: 72
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

### *Volodymyr Kuzma, Anton Mazuryk, Oleksii Mytnyk*

## Introduction

**Naive Bayes classifier** is a probabilistic classifier whose aim is to
determine which class some observation probably belongs to by using the
Bayes formula:
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong independence assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given
observation. Thus, $\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now
can be calculated as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}$$Hence,
$$\mathsf{P}(\mathrm{spam} \mid \mathrm{message})=\mathsf{P}(spam)\times\prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{word}_i\mid \mathrm{spam})}{\mathsf{P}(\mathrm{word}_i)},\ \mathsf{P}(\mathrm{spam})=\frac{\mathrm{M}_s}{\mathrm{M}}$$

$$
\mathsf{P}(\mathrm{word}_i)=\frac{\mathrm{W}_{\mathrm{word}_i}}{\mathrm{W}},
\mathsf{P}(\mathrm{word}_i\mid \mathrm{spam})=\frac{\mathrm{S}_{\mathrm{word}_i}}{\mathrm{S}}
$$

Since, for ham $M$ and $\mathsf{P}(\mathrm{word}_i)$ are the same, we
should compare

$\mathrm{M}_s\times$

**Necessary libraries**

```{r}

library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
library(wordcloud)
```

### Data pre-processing

```{r}
list.files(getwd()) # Should print "bayes_classifier.Rmd" "data" "README.md"
list.files("data")  # Should print "stop_words.txt" "test.csv" "train.csv" 
```

```{r}
test_path <- "data/test.csv"
train_path <- "data/train.csv"

stop_words <- strsplit(read_file("data/stop_words.txt"), split='\n')[[1]]
```

#### Fetching data from csv

```{r}
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test  <- read.csv(file = test_path,  stringsAsFactors = FALSE)
```

#### Filtering

```{r}
tidy_text <- train %>% mutate(ID = row_number()) %>% unnest_tokens('Word', 'Message', token="words") %>% filter(!Word %in% stop_words)
```

#### **Final train data**

```{r}
X_train <- tidy_text %>% select(Word, ID)
Y_train <- (tidy_text$Category == "spam") %>% ifelse(1, 0)
```

## Data visualization

**Common spam words:**

```{r}
count_df <- tidy_text %>% filter(Category == "spam") %>% count(Word)
wordcloud(words = count_df$Word, freq = count_df$n, max.words = 80)
```

**Common ham words:**

```{r}
count_df <- tidy_text %>% filter(Category == "ham") %>% count(Word)
wordcloud(words = count_df$Word, freq = count_df$n, max.words = 80)
```

## Classifier implementation

```{r}
naiveBayes <- setRefClass("naiveBayes",
  fields = list(
    spam_messages_cnt="number",
    ham_messages_cnt="number",
    spam_words_cnt="number",
    ham_words_cnt="number",
    words="data.frame"
    ),
  methods = list(
    fit = function(X, y)
    {
      df <- data.frame(X, y)
      spam_messages_cnt <<- sum((df[!duplicated(df$ID), ])$y == 1)
      ham_messages_cnt <<- sum((df[!duplicated(df$ID), ])$y == 0)
      words <<- df %>% group_by(Word) %>% transmute(
        Ham = sum(1 - y), Spam = sum(y))
      words <<- words[!duplicated(words$Word), ]
      spam_words_cnt <<- sum(words$Spam)
      ham_words_cnt <<- sum(words$Ham)
    },
    
    predict = function(message, threshold)
    {
      message <- tibble('Message' = message) %>%
                  unnest_tokens('Word', 'Message', token="words") %>%
                  filter(!Word %in% stop_words) %>%
                  merge(words, by = "Word", all.x = TRUE)
      message[is.na(message)] = 0
      
      probability <- message %>% transmute(
        pspam = (Spam + 1) / (nrow(words) + spam_words_cnt),
        pham = (Ham + 1) / (nrow(words) + ham_words_cnt))
      
      # In the end, we simplify the probabilities through removing denominators as they are the same
      # The below inequality means that normalized (meaning p_spam/(p_spam + p_ham)) chance
      # of message being spam is greater than the threshold:
      # p_spam/(p_spam + p_ham) > k -> p_spam*(1-k) > p_ham*k -> p_spam > (k/(1-k))*p_ham
      
      return (prod(probability$pspam) * spam_messages_cnt >
              (threshold / (1 - threshold)) * prod(probability$pham) * ham_messages_cnt)
    },
    
    score = function(X_test, y_test, threshold = 0.5)
    {
      data <- data.frame(
        X_test %>% group_by(Message) %>% transmute(Predicted=predict(Message, threshold)),
        y_test %>% transmute(Actual = ifelse(Category == "spam", TRUE, FALSE)))
      
      tp <- nrow(data[data$Predicted == TRUE & data$Actual == TRUE,])
      tn <- nrow(data[data$Predicted == FALSE & data$Actual == FALSE,])
      fp <- nrow(data[data$Predicted == FALSE & data$Actual == TRUE,])
      fn <- nrow(data[data$Predicted == TRUE & data$Actual == FALSE,])
      precision <- tp / (tp + fp)
      recall <- tp / (tp + fn)
      f1 <- (2 * precision * recall / (precision + recall))
      
      return (list(f1, precision, recall))
    }
))

model <- naiveBayes()
model$fit(X_train, Y_train)
model$score(test["Message"], test["Category"])[1]
```

## Measure effectiveness of your classifier

-   Note that accuracy is not always a good metric for your classifier.
    Look at precision and recall curves, F1 score metric.
-   Visualize them.
-   Show failure cases.

```{r}
x = seq(0, 1, length = 20)
y = sapply(x, model$score, X_test = test["Message"], y_test = test["Category"])

Precision = y[,2]
Recall = y[,3]

plot(Recall, Precision, xlab = "Recall", ylab = "Precision", ylim = c(0,1), xlim = c(0,1))
```

## Conclusions

Summarize your work by explaining in a few sentences the points listed
below.

-   Describe the method implemented in general. Show what are
    mathematical foundations you are basing your solution on.
-   List pros and cons of the method. This should include the
    limitations of your method, all the assumption you make about the
    nature of your data etc.
