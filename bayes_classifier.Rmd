---
editor_options:
  markdown:
    wrap: 72
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

### *Volodymyr Kuzma, Anton Mazuryk, Oleksii Mytnyk*

## Introduction

**Naive Bayes classifier** is a probabilistic classifier whose aim is to
determine which class some observation probably belongs to by using the
Bayes formula:
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong independence assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given
observation. Thus, $\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now
can be calculated as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}$$Hence,
$$\mathsf{P}(\mathrm{spam} \mid \mathrm{message})=\mathsf{P}(spam)\times\prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{word}_i\mid \mathrm{spam})}{\mathsf{P}(\mathrm{word}_i)},\ \mathsf{P}(\mathrm{spam})=\frac{\mathrm{M}_s}{\mathrm{M}}$$

$$
\mathsf{P}(\mathrm{word}_i)=\frac{\mathrm{W}_{\mathrm{word}_i}}{\mathrm{W}},
\mathsf{P}(\mathrm{word}_i\mid \mathrm{spam})=\frac{\mathrm{S}_{\mathrm{word}_i}}{\mathrm{S}}
$$

Since, for ham $M$ and $\mathsf{P}(\mathrm{word}_i)$ are the same, we
should compare

$\mathrm{M}_s\times$

**Necessary libraries**

```{r}

library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
library(wordcloud)
```

### Data pre-processing

```{r}
list.files(getwd()) # Should print "bayes_classifier.Rmd" "data" "README.md"
list.files("data")  # Should print "stop_words.txt" "test.csv" "train.csv" 
```

```{r}
test_path <- "data/test.csv"
train_path <- "data/train.csv"

stop_words <- strsplit(read_file("data/stop_words.txt"), split='\n')[[1]]
```

#### Fetching data from csv

```{r}
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test  <- read.csv(file = test_path,  stringsAsFactors = FALSE)
```

#### Filtering

```{r}
tidy_text <- train %>% mutate(ID = row_number()) %>% unnest_tokens('Word', 'Message', token="words") %>% filter(!Word %in% stop_words)
```

#### **Final train data**

```{r}
X_train <- tidy_text %>% select(Word, ID)
Y_train <- (tidy_text$Category == "spam") %>% ifelse(1, 0)
```

## Data visualization

**Common spam words:**

```{r}
count_df <- tidy_text %>% filter(Category == "spam") %>% count(Word)
wordcloud(words = count_df$Word, freq = count_df$n, max.words = 80)
```

**Common ham words:**

```{r}
count_df <- tidy_text %>% filter(Category == "ham") %>% count(Word)
wordcloud(words = count_df$Word, freq = count_df$n, max.words = 80)
```

## Classifier implementation

```{r}
naiveBayes <- setRefClass("naiveBayes",
  fields = list(
    spam_messages_cnt="number",
    ham_messages_cnt="number",
    spam_words_cnt="number",
    ham_words_cnt="number",
    words="data.frame"
    ),
  methods = list(
    fit = function(X, y)
    {
      df <- data.frame(X, y)
      spam_messages_cnt <<- sum((df[!duplicated(df$ID), ])$y == 1)
      ham_messages_cnt <<- sum((df[!duplicated(df$ID), ])$y == 0)
      words <<- df %>% group_by(Word) %>% transmute(
        Ham = sum(1 - y), Spam = sum(y))
      words <<- words[!duplicated(words$Word), ]
      spam_words_cnt <<- sum(words$Spam)
      ham_words_cnt <<- sum(words$Ham)
    },
    
    predict = function(message, threshold)
    {
      message <- tibble('Message' = message) %>%
                  unnest_tokens('Word', 'Message', token="words") %>%
                  filter(!Word %in% stop_words) %>%
                  merge(words, by = "Word", all.x = TRUE)
      message[is.na(message)] = 0
      
      probability <- message %>% transmute(
        pspam = (Spam + 1) / (nrow(words) + spam_words_cnt),
        pham = (Ham + 1) / (nrow(words) + ham_words_cnt))
      
      # In the end, we simplify the probabilities through removing denominators as they are the same
      # The below inequality means that normalized (meaning p_spam/(p_spam + p_ham)) chance
      # of message being spam is greater than the threshold:
      # p_spam/(p_spam + p_ham) > k -> p_spam*(1-k) > p_ham*k -> p_spam > (k/(1-k))*p_ham
      
      return (prod(probability$pspam) * spam_messages_cnt >
              (threshold / (1 - threshold)) * prod(probability$pham) * ham_messages_cnt)
    },
    
    score = function(X_test, y_test, threshold = 0.5)
    {
      data <- data.frame(
        X_test %>% group_by(Message) %>% transmute(Predicted=predict(Message, threshold)),
        y_test %>% transmute(Actual = ifelse(Category == "spam", TRUE, FALSE)))
      
      tp <- nrow(data[data$Predicted == TRUE & data$Actual == TRUE,])
      tn <- nrow(data[data$Predicted == FALSE & data$Actual == FALSE,])
      fp <- nrow(data[data$Predicted == FALSE & data$Actual == TRUE,])
      fn <- nrow(data[data$Predicted == TRUE & data$Actual == FALSE,])
      precision <- tp / (tp + fp)
      recall <- tp / (tp + fn)
      f1 <- (2 * precision * recall / (precision + recall))
      
      return (list(f1, precision, recall))
    }
))

model <- naiveBayes()
model$fit(X_train, Y_train)
f1_score = model$score(test["Message"], test["Category"])[1]
```

## Measure effectiveness of your classifier

-   Note that accuracy is not always a good metric for your classifier.
    Look at precision and recall curves, F1 score metric.
-   Visualize them.
-   Show failure cases.

### F1 score

Calculated for a threshold of 0.5. It is a harmonic mean of precision
and recall, so its high value corresponds to the high value of both
parameters (it can also be seen on the recall-precision curve), meaning
that the classifier has a low rate of both incorrect positive and negative predictions

```{r}
f1_score
```

### Data receiving for plots and lists of failures

(lasts nearly 3 minutes and is unnecessary if the variables are already
saved)

```{r}
x <- seq(0.05, 0.95, length = 19)
x <- append(x, seq(0.0025, 0.01, length = 4)) 

y <- sapply(x, model$score, X_test = test["Message"], y_test = test["Category"])

Precision <- y[2,]
Recall <- y[3,]


data <- data.frame(
        test["Message"] %>% group_by(Message) %>% transmute(Predicted = model$predict(Message, threshold = 0.5)),
        test["Category"] %>% transmute(Actual = ifelse(Category == "spam", TRUE, FALSE)))
```

### Precision for different thresholds

```{r}
plot(x, Precision, xlab = "Threshold", ylab = "Precision")
```

### Recall for different thresholds

```{r}
plot(x, Recall, xlab = "Threshold", ylab = "Recall")
```

### Precision-recall curve

Both recall and precision have high values.

```{r}
plot(Recall, Precision, xlab = "Recall", ylab = "Precision", ylim = c(0.5,1), xlim = c(0,1))
```

### Some false negative cases

```{r}
fp <- data[data$Predicted == FALSE & data$Actual == TRUE, "Message"]
fp
```

### Some false positive cases

```{r}
fn <- data[data$Predicted == TRUE & data$Actual == FALSE,"Message"]
fn
```

## Conclusions

Summarize your work by explaining in a few sentences the points listed
below.

-   Describe the method implemented in general. Show what are
    mathematical foundations you are basing your solution on.
    The method is quite simple: we use the Bayes' theorem to find the probability that message belongs to a certain class (spam) by calculating the chance of the message appearing if the class is already known. The mathematical foundation is (as it was already said) the Bayes' theorem, on which we base the main "predict" function, and the Laplace smoothing, used to prevent errors due to the possible zero-probabilities. There are also several smaller tricks such as (1)reducing the fraction of the probability to the terms which are different in spam_probability and non-spam_probability, as we need it only for comparison or (2)proportionally increasing the chances for their sum to become 1 to be able to use a threshold coefficient.

-   List pros and cons of the method. This should include the
    limitations of your method, all the assumption you make about the
    nature of your data etc.
    The main advantages of the method are its simplicity and sufficient accuracy (which depends on the data set but may be high as shown above). The biggest assumption (and the disadvantage that it creates) is the assumption of independence of separate words belonging to the message. It means that we assume that the words do not build logical connections that make the chance of belonging to the class higher, which is obviously not correct, yet due to the existence of common spam topics, each word with its independent from the context meaning has a significant chance of being located in such a topic. A big limitation is that the classifier needs a balanced data set. The imbalanced one will lead to good accuracy on this exact data set (its testing part) yet quite low recall and precision rates which are much more important when it is expected to be able to analyze fresh data from another source.
