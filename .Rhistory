demo()
log(100); log10(100); log(100,10)     # logarithms
sin(pi/2); tan(pi/2)                  # trig
sqrt(1/2^2)                           # squares and roots
exp(cos(pi/2))                        # exponents
knitr::opts_chunk$set(echo = TRUE)
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- read_file("data/stop_words.txt")
list.files(getwd())
list.files("data")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- read.file("data/stop_words.txt")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- read.file("data/stop_words.txt")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- read.file("data/stop_words.txt")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- read.file("data/stop_words.txt")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- read.file("data/stop_words.txt")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- read.file("data/stop_words.txt")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- read.file("data/stop_words.txt")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- read.file("data/stop_words.txt")
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- read_file("data/stop_words.txt")
# https://stackoverflow.com/questions/27195912/why-does-strsplit-return-a-list
splitted_stop_words <- strsplit(stop_words, split='\n')
splitted_stop_words <- splitted_stop_words[[1]]
train <-  read.csv(file = train_path, stringsAsFactors = FALSE)
test <-  read.csv(file = test_path, stringsAsFactors = FALSE)
# note the power functional features of R bring us!
tidy_text <- unnest_tokens(train, 'splitted', 'text', token="words") %>%
filter(!splitted %in% stop_words)
# note the power functional features of R bring us!
tidy_text <- unnest_tokens(train, 'splitted', 'text', token="message") %>%
filter(!splitted %in% stop_words)
# note the power functional features of R bring us!
tidy_text <- unnest_tokens(train, 'splitted', 'text', token="words") %>%
filter(!splitted %in% stop_words)
# note the power functional features of R bring us!
tidy_text <- unnest_tokens(train, 'splitted', 'message', token="words") %>%
filter(!splitted %in% stop_words)
# note the power functional features of R bring us!
tidy_text <- unnest_tokens(train, 'splitted', 'message', token="words") %>%
filter(!splitted %in% stop_words)
# note the power functional features of R bring us!
tidy_text <- unnest_tokens(train, 'splitted', 'message', token="words") %>%
filter(!splitted %in% stop_words)
# note the power functional features of R bring us!
tidy_text <- unnest_tokens(train, 'splitted', 'Message', token="words") %>%
filter(!splitted %in% stop_words)
tidy_text %>% count(splitted,sort=TRUE)
# note the power functional features of R bring us!
tidy_text <- unnest_tokens(train, 'splitted', 'Message', token="words") %>%
filter(!splitted %in% splitted_stop_words)
tidy_text %>% count(splitted,sort=TRUE)
knitr::opts_chunk$set(echo = TRUE)
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
list.files(getwd())
list.files("data")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- strsplit(read_file("data/stop_words.txt"), split='\n')[[1]]
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test  <- read.csv(file = test_path, stringsAsFactors = FALSE)
tidy_text <- unnest_tokens(train, 'splitted', 'Message', token="words") %>%
filter(!splitted %in% splitted_stop_words)
tidy_text %>% count(splitted,sort=TRUE)
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of wards!
fields = list(),
methods = list(
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y)
{
# TODO
},
# return prediction for a single message
predict = function(message)
{
# TODO
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test)
{
# TODO
}
))
model = naiveBayes()
model$fit()
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- strsplit(read_file("data/stop_words.txt"), split='\n')[[1]]
knitr::opts_chunk$set(echo = TRUE)
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
list.files(getwd())
list.files("data")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- strsplit(read_file("data/stop_words.txt"), split='\n')[[1]]
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test  <- read.csv(file = test_path, stringsAsFactors = FALSE)
tidy_text <- unnest_tokens(train, 'splitted', 'Message', token="words") %>%
filter(!splitted %in% splitted_stop_words)
knitr::opts_chunk$set(echo = TRUE)
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
list.files(getwd())
list.files("data")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- strsplit(read_file("data/stop_words.txt"), split='\n')[[1]]
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test  <- read.csv(file = test_path, stringsAsFactors = FALSE)
tidy_text <- unnest_tokens(train, 'splitted', 'Message', token="words") %>%
filter(!splitted %in% stop_words)
tidy_text %>% count(splitted,sort=TRUE)
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of wards!
fields = list(),
methods = list(
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y)
{
# TODO
},
# return prediction for a single message
predict = function(message)
{
# TODO
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test)
{
# TODO
}
))
model = naiveBayes()
model$fit()
View(tidy_text)
View(tidy_text)
View(test)
tidy_text <- unnest_tokens(train, 'Words', 'Message', token="words") %>%
filter(!splitted %in% stop_words)
tidy_text <- unnest_tokens(train, 'Words', 'Message', token="words") %>%
filter(!Words %in% stop_words)
tidy_text %>% count(splitted,sort=TRUE)
View(train)
View(train)
View(tidy_text)
tidy_text <- unnest_tokens(train, 'Word', 'Message', token="words") %>%
filter(!Word %in% stop_words)
tidy_text %>% count(Word, sort=TRUE)
tidy_text <- unnest_tokens(train, 'Word', 'Message', token="words") %>%
filter(!Word %in% stop_words)
tidy_text %>% count(Word, sort=TRUE)
gc()
knitr::opts_chunk$set(echo = TRUE)
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
list.files(getwd())
list.files("data")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- strsplit(read_file("data/stop_words.txt"), split='\n')[[1]]
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test  <- read.csv(file = test_path, stringsAsFactors = FALSE)
tidy_text <- unnest_tokens(train, 'Word', 'Message', token="words") %>%
filter(!Word %in% stop_words)
tidy_text %>% count(Word, sort=TRUE)
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of wards!
fields = list(),
methods = list(
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y)
{
# TODO
},
# return prediction for a single message
predict = function(message)
{
# TODO
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test)
{
# TODO
}
))
model = naiveBayes()
model$fit()
View(tidy_text)
View(tidy_text)
knitr::opts_chunk$set(echo = TRUE)
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
list.files(getwd())
list.files("data")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- strsplit(read_file("data/stop_words.txt"), split='\n')[[1]]
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test  <- read.csv(file = test_path, stringsAsFactors = FALSE)
train_data <- unnest_tokens(train, 'Word', 'Message', token="words") %>%
filter(!Word %in% stop_words)
train_data %>% count(Word, sort=TRUE)
naiveBayes <- setRefClass("naiveBayes",
# here it would be wise to have some vars to store intermediate result
# frequency dict etc. Though pay attention to bag of wards!
fields = list(),
methods = list(
# prepare your training data as X - bag of words for each of your
# messages and corresponding label for the message encoded as 0 or 1
# (binary classification task)
fit = function(X, y)
{
# TODO
},
# return prediction for a single message
predict = function(message)
{
# TODO
},
# score you test set so to get the understanding how well you model
# works.
# look at f1 score or precision and recall
# visualize them
# try how well your model generalizes to real world data!
score = function(X_test, y_test)
{
# TODO
}
))
model = naiveBayes()
model$fit()
knitr::opts_chunk$set(echo = TRUE)
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
list.files(getwd())
list.files("data")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- strsplit(read_file("data/stop_words.txt"), split='\n')[[1]]
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test  <- read.csv(file = test_path, stringsAsFactors = FALSE)
train_data <- unnest_tokens(train, 'Word', 'Message', token="words") %>%
filter(!Word %in% stop_words)
X_train <- train_data$Word
Y_train <- train_data$Category %>% map(~ x - 1000)
knitr::opts_chunk$set(echo = TRUE)
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
list.files(getwd())
list.files("data")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- strsplit(read_file("data/stop_words.txt"), split='\n')[[1]]
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test  <- read.csv(file = test_path, stringsAsFactors = FALSE)
train_data <- unnest_tokens(train, 'Word', 'Message', token="words") %>%
filter(!Word %in% stop_words)
X_train <- train_data$Word
Y_train <- train_data$Category %>% { map(~ x - 1000) }
knitr::opts_chunk$set(echo = TRUE)
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
list.files(getwd())
list.files("data")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- strsplit(read_file("data/stop_words.txt"), split='\n')[[1]]
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test  <- read.csv(file = test_path, stringsAsFactors = FALSE)
train_data <- unnest_tokens(train, 'Word', 'Message', token="words") %>%
filter(!Word %in% stop_words)
X_train <- train_data$Word
Y_train <- train_data$Category %>% map_values(~ x - 1000)
knitr::opts_chunk$set(echo = TRUE)
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
list.files(getwd())
list.files("data")
test_path <- "data/test.csv"
train_path <- "data/train.csv"
stop_words <- strsplit(read_file("data/stop_words.txt"), split='\n')[[1]]
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test  <- read.csv(file = test_path, stringsAsFactors = FALSE)
train_data <- unnest_tokens(train, 'Word', 'Message', token="words") %>%
filter(!Word %in% stop_words)
X_train <- train_data$Word
Y_train <- train_data$Category %>% map_data(~ x == "spam")
